{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2888672f",
      "metadata": {
        "id": "2888672f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from scipy import signal\n",
        "import librosa\n",
        "import time\n",
        "from scipy.io import wavfile\n",
        "\n",
        "!wget https://digitalmusicprocessing.github.io/HW6_StringAlong/data.wav\n",
        "!wget https://digitalmusicprocessing.github.io/HW6_StringAlong/marvin.wav\n",
        "!wget https://digitalmusicprocessing.github.io/HW6_StringAlong/adele.wav\n",
        "\n",
        "!pip install torchcrepe\n",
        "import torchcrepe # https://github.com/maxrmorrison/torchcrepe"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07c165d6",
      "metadata": {
        "id": "07c165d6"
      },
      "source": [
        "# Utility Functions (Given)\n",
        "\n",
        "General purpose functions that will help us with specific parts of the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3771cec9",
      "metadata": {
        "id": "3771cec9"
      },
      "outputs": [],
      "source": [
        "def upsample_time(X, hop_length, mode='nearest'):\n",
        "    \"\"\"\n",
        "    Upsample a tensor by a factor of hop_length along the time axis\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X: torch.tensor(M, T, N)\n",
        "        A tensor in which the time axis is axis 1\n",
        "    hop_length: int\n",
        "        Upsample factor\n",
        "    mode: string\n",
        "        Mode of interpolation.  'nearest' by default to avoid artifacts\n",
        "        where notes in the violin jump by large intervals\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.tensor(M, T*hop_length, N)\n",
        "        Upsampled tensor\n",
        "    \"\"\"\n",
        "    X = X.permute(0, 2, 1)\n",
        "    X = nn.functional.interpolate(X, size=hop_length*X.shape[-1], mode=mode)\n",
        "    return X.permute(0, 2, 1)\n",
        "\n",
        "def fftconvolve(x, h):\n",
        "    \"\"\"\n",
        "    Perform a fast convolution of two tensors across their last axis\n",
        "    by using the FFT. Since the DFT assumes circularity, zeropad them\n",
        "    appropriately before doing the FFT and slice them down afterwards\n",
        "\n",
        "    The length of the result will be equivalent to np.convolve's 'same'\n",
        "\n",
        "    Refer to this module for more background:\n",
        "    https://ursinus-cs372-s2023.github.io/Modules/Module14/Video4\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x: torch.tensor(..., N1)\n",
        "        First tensor\n",
        "    h: torch.tensor(..., N2)\n",
        "        Second tensor\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.tensor(..., max(N1, N2))\n",
        "    Tensor resulting from the convolution of x and y across their last axis,\n",
        "    \"\"\"\n",
        "    N = max(x.shape[-1], h.shape[-1])\n",
        "    if x.shape[-1] != h.shape[-1]:\n",
        "        # Zeropad so they're equal\n",
        "        if x.shape[-1] < N:\n",
        "            x = nn.functional.pad(x, (0, N-x.shape[-1]))\n",
        "        if h.shape[-1] < N:\n",
        "            h = nn.functional.pad(h, (0, N-h.shape[-1]))\n",
        "    x = nn.functional.pad(x, (0, N))\n",
        "    h = nn.functional.pad(h, (0, N))\n",
        "    X = torch.fft.rfft(x)\n",
        "    H = torch.fft.rfft(h)\n",
        "    y = torch.fft.irfft(X*H)\n",
        "    return y[..., 0:N]\n",
        "\n",
        "\n",
        "def plot_stft_comparison(F, L, X, Y, reverb, losses=torch.tensor([]), win=1024, sr=16000):\n",
        "    \"\"\"\n",
        "    Some code to help compare the STFTs of ground truth and output audio, while\n",
        "    also plotting the frequency, loudness, and reverb to get an idea of what the\n",
        "    inputs to the network were that gave rise to these ouputs.  It's very helpful\n",
        "    to call this method while monitoring the training of the network\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    F: torch.tensor(n_batches, n_samples/hop_length, 1)\n",
        "         Tensor holding the pitch estimates for the clips\n",
        "    L: torch.tensor(n_batches, n_samples/hop_length, 1)\n",
        "         Tensor holding the loudness estimates for the clips\n",
        "    X: torch.tensor(n_batches, n_samples, 1)\n",
        "        Ground truth audio\n",
        "    Y: torch.tensor(n_batches, n_samples, 1)\n",
        "        Output audio from the network->decoder\n",
        "    reverb: torch.tensor(reverb_len)\n",
        "        The learned reverb\n",
        "    losses: list\n",
        "        A list of losses over epochs over time\n",
        "    win: int\n",
        "        Window length to use in the STFT\n",
        "    sr: int\n",
        "        Sample rate of audio (used to help make proper units for time and frequency)\n",
        "    \"\"\"\n",
        "    hop = 256\n",
        "    hann = torch.hann_window(win).to(X)\n",
        "    SX = torch.abs(torch.stft(X.squeeze(), win, hop, win, hann, return_complex=True))\n",
        "    SY = torch.abs(torch.stft(Y.squeeze(), win, hop, win, hann, return_complex=True))\n",
        "    print(SX.shape)\n",
        "    extent = (0, SX.shape[2]*hop/sr, SX.shape[1]*sr/win, 0)\n",
        "    plt.subplot(321)\n",
        "    plt.imshow(torch.log10(SX.detach().cpu()[0, :, :]), aspect='auto', cmap='magma', extent=extent)\n",
        "    plt.title(\"Ground Truth\")\n",
        "    plt.ylim([0, 8000])\n",
        "    plt.xlabel(\"Time (Sec)\")\n",
        "    plt.ylabel(\"Frequency (hz)\")\n",
        "\n",
        "    plt.subplot(322)\n",
        "    plt.imshow(torch.log10(SY.detach().cpu()[0, :, :]), aspect='auto', cmap='magma', extent=extent)\n",
        "    plt.title(\"Synthesized\")\n",
        "    plt.ylim([0, 8000])\n",
        "    plt.xlabel(\"Time (Sec)\")\n",
        "    plt.ylabel(\"Frequency (hz)\")\n",
        "\n",
        "    plt.subplot(323)\n",
        "    plt.plot(F.detach().cpu()[0, :, 0])\n",
        "    plt.title(\"Fundamental Frequency\")\n",
        "    plt.xlabel(\"Window index\")\n",
        "    plt.ylabel(\"Hz\")\n",
        "    plt.subplot(324)\n",
        "    plt.plot(L.detach().cpu()[0, :, 0])\n",
        "    plt.title(\"Loudness\")\n",
        "    plt.xlabel(\"Window Index\")\n",
        "    plt.ylabel(\"Z-normalized dB\")\n",
        "    if torch.numel(losses) > 0:\n",
        "        plt.subplot(325)\n",
        "        plt.plot(losses.detach().cpu().numpy().flatten())\n",
        "        plt.yscale(\"log\")\n",
        "        plt.title(\"Losses (Current {:.3f})\".format(losses[-1]))\n",
        "        plt.xlabel(\"Epoch\")\n",
        "    plt.subplot(326)\n",
        "    plt.plot(reverb.detach().cpu().flatten())\n",
        "    plt.title(\"Impulse Response\")\n",
        "    plt.xlabel(\"Sample index\")\n",
        "\n",
        "################################################\n",
        "# Loudness code modified from original Google Magenta DDSP implementation in tensorflow\n",
        "# https://github.com/magenta/ddsp/blob/86c7a35f4f2ecf2e9bb45ee7094732b1afcebecd/ddsp/spectral_ops.py#L253\n",
        "# which, like this repository, is licensed under Apache2 by Google Magenta Group, 2020\n",
        "# Modifications by Chris Tralie, 2023\n",
        "\n",
        "def power_to_db(power, ref_db=0.0, range_db=80.0, use_tf=True):\n",
        "    \"\"\"Converts power from linear scale to decibels.\"\"\"\n",
        "    # Convert to decibels.\n",
        "    db = 10.0*np.log10(np.maximum(power, 10**(-range_db/10)))\n",
        "    # Set dynamic range.\n",
        "    db -= ref_db\n",
        "    db = np.maximum(db, -range_db)\n",
        "    return db\n",
        "\n",
        "def extract_loudness(x, sr, hop_length, n_fft=512):\n",
        "    \"\"\"\n",
        "    Extract the loudness in dB by using an A-weighting of the power spectrum\n",
        "    (section B.1 of the paper)\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    x: ndarray(N)\n",
        "        Audio samples\n",
        "    sr: int\n",
        "        Sample rate (used to figure out frequencies for A-weighting)\n",
        "    hop_length: int\n",
        "        Hop length between loudness estimates\n",
        "    n_fft: int\n",
        "        Number of samples to use in each window\n",
        "    \"\"\"\n",
        "    # Computed centered STFT\n",
        "    S = librosa.stft(x, n_fft=n_fft, hop_length=hop_length, win_length=n_fft, center=True)\n",
        "\n",
        "    # Compute power spectrogram\n",
        "    amplitude = np.abs(S)\n",
        "    power = amplitude**2\n",
        "\n",
        "    # Perceptual weighting.\n",
        "    freqs = np.arange(S.shape[0])*sr/n_fft\n",
        "    a_weighting = librosa.A_weighting(freqs)[:, None]\n",
        "\n",
        "    # Perform weighting in linear scale, a_weighting given in decibels.\n",
        "    weighting = 10**(a_weighting/10)\n",
        "    power = power * weighting\n",
        "\n",
        "    # Average over frequencies (weighted power per a bin).\n",
        "    avg_power = np.mean(power, axis=0)\n",
        "    loudness = power_to_db(avg_power)\n",
        "    return np.array(loudness, dtype=np.float32)\n",
        "\n",
        "################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7de4d056",
      "metadata": {
        "id": "7de4d056"
      },
      "source": [
        "# Part 1: Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ae04bea",
      "metadata": {
        "id": "6ae04bea"
      },
      "source": [
        "## FM Synthesis Dataset (Given)\n",
        "\n",
        "For debugging, if you need it.  Your network should at least be able to learn these very simple sounds, so if it can't, you should figure out what the problem is before you move onto training your network on the real violin audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f86ec3b8",
      "metadata": {
        "id": "f86ec3b8"
      },
      "outputs": [],
      "source": [
        "class FMDataset(Dataset):\n",
        "    def __init__(self, sr, hop_length, samples_per_batch=1000):\n",
        "        \"\"\"\n",
        "        Instantiate an fm dataset\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        sr: int\n",
        "            Sample rate\n",
        "        hop_length: int\n",
        "            Samples between loudness and pitch frames\n",
        "        samples_per_batch: int\n",
        "            The length of this object\n",
        "        \"\"\"\n",
        "        self.sr = sr\n",
        "        self.hop_length = hop_length\n",
        "        self.samples_per_batch = samples_per_batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.samples_per_batch\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Generate a random FM plucked string note between A3 and A5\n",
        "        over a duration of 4 seconds\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        idx: int\n",
        "            Index of example (ignored because data is random)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        x: ndarray(sr*4)\n",
        "            Audio samples\n",
        "        pitch: ndarray(sr*4//hop_length)\n",
        "            The pitch (a constant line since this is one solid note)\n",
        "        loudness: ndarray(sr*4//hop_length)\n",
        "            Loudness\n",
        "        \"\"\"\n",
        "        note = np.random.randint(-12, 12)\n",
        "        sr = self.sr\n",
        "        ratio = 1\n",
        "        I = 8\n",
        "        lam = 3\n",
        "        duration = 4\n",
        "        envelope = lambda N, sr, lam: np.exp(-lam*np.arange(N)/sr)\n",
        "        N = int(duration*sr)\n",
        "        ts = np.arange(N)/sr\n",
        "        f = 440*2**(note/12)\n",
        "        fm = f*ratio\n",
        "        x = np.cos(2*np.pi*f*ts + envelope(N, sr, lam)*I*(np.cos(2*np.pi*fm*ts)))\n",
        "        loudness = envelope(N, sr, lam)\n",
        "        x = x*loudness\n",
        "        K = x.size//self.hop_length\n",
        "        loudness = np.array(loudness[0::self.hop_length], dtype=np.float32)\n",
        "        loudness = 10*np.log10(loudness**2+1e-8)\n",
        "        loudness = torch.from_numpy(loudness).view(K, 1)\n",
        "        x = torch.from_numpy(x).view(x.size, 1)\n",
        "        # Extract pitch and loudness\n",
        "        pitch = 440*(2**(note/12))*torch.ones(K)\n",
        "        pitch = pitch.view(K, 1)\n",
        "        return x, pitch, loudness\n",
        "\n",
        "## An example of a dataset\n",
        "sr = 16000\n",
        "dataset = FMDataset(sr, 160)\n",
        "loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "(X, F, L) = next(iter(loader))\n",
        "ipd.Audio(X[0, :, 0], rate=sr)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db8eadec",
      "metadata": {
        "id": "db8eadec"
      },
      "source": [
        "## Instrument Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30f8a8db",
      "metadata": {
        "id": "30f8a8db"
      },
      "outputs": [],
      "source": [
        "## TODO: Fill this in!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5fbd3dc",
      "metadata": {
        "id": "c5fbd3dc"
      },
      "source": [
        "# Part 2a: Decoder Architecture\n",
        "\n",
        "Section B.2 of the paper\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f44df89",
      "metadata": {
        "id": "4f44df89"
      },
      "outputs": [],
      "source": [
        "def modified_sigmoid(x):\n",
        "    return 2*torch.sigmoid(x)**np.log(10) + 1e-7\n",
        "\n",
        "## TODO: Fill this in!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d9af7e3",
      "metadata": {
        "id": "0d9af7e3"
      },
      "source": [
        "# Part 2b: Synthesizer\n",
        "Section 3.2, 3.3 B.5\n",
        "\n",
        "Use the outputs of the decoder network to create audio samples, *using only torch methods*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0e8b320",
      "metadata": {
        "id": "f0e8b320"
      },
      "source": [
        "## Subtractive Synthesizer (Given)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c013c9fc",
      "metadata": {
        "id": "c013c9fc"
      },
      "outputs": [],
      "source": [
        "def subtractive_synthesis(S, hop_length):\n",
        "    \"\"\"\n",
        "    Perform subtractive synthesis by converting frequency domain transfer\n",
        "    functions into causal, zero-phase, windowed impulse responses\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    S: n_batches x time x n_bands\n",
        "        Subtractive synthesis parameters\n",
        "    hop_length: int\n",
        "        Hop length between subtractive synthesis windows\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    torch.tensor(n_batches, time*hop_length, 1)\n",
        "        Subtractive synthesis audio components for each clip\n",
        "    \"\"\"\n",
        "\n",
        "    # Put an imaginary component of all 0s across a new last axis\n",
        "    # https://pytorch.org/docs/stable/generated/torch.view_as_complex.html\n",
        "    S = torch.stack([S, torch.zeros_like(S)], -1)\n",
        "    S = torch.view_as_complex(S)\n",
        "    # Do the inverse real DFT (assuming symmetry)\n",
        "    h = torch.fft.irfft(S)\n",
        "\n",
        "    # Shift the impulse response to zero-phase\n",
        "    nh = h.shape[-1]\n",
        "    h = torch.roll(h, nh//2, -1)\n",
        "    # Apply hann window\n",
        "    h = h*torch.hann_window(nh, dtype=h.dtype, device=h.device)\n",
        "    # Shift back to causal\n",
        "    h = nn.functional.pad(h, (0, hop_length-nh))\n",
        "    h = torch.roll(h, -nh//2, -1)\n",
        "\n",
        "    # Apply the impulse response to random noise in [-1, 1]\n",
        "    noise = torch.rand(h.shape[0],h.shape[1],hop_length).to(h.device)\n",
        "    noise = noise*2 - 1\n",
        "    noise = fftconvolve(noise, h).contiguous()\n",
        "\n",
        "    # Flatten nonoverlapping samples to one contiguous stream\n",
        "    return noise.reshape(noise.shape[0], noise.shape[1]*noise.shape[2], 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "214f6cab",
      "metadata": {
        "id": "214f6cab"
      },
      "source": [
        "## Additive Synthesizer / Putting It Together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a141f30d",
      "metadata": {
        "id": "a141f30d"
      },
      "outputs": [],
      "source": [
        "## TODO: Fill this in!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33b4b192",
      "metadata": {
        "id": "33b4b192"
      },
      "source": [
        "# Part 3: Loss Function\n",
        "\n",
        "Implement Multi-Scale Spectral Loss (DDSP Section 4.2.1)\n",
        "\n",
        "Use torch.stft to help you.  Don't forget to squeeze() the input to the STFT to get rid of the singleton dimension at the end\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "278a3c83",
      "metadata": {
        "id": "278a3c83"
      },
      "outputs": [],
      "source": [
        "## TODO: Fill this in!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "140e4e7b",
      "metadata": {
        "id": "140e4e7b"
      },
      "source": [
        "# Part 4: Testing Example Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d5a333d",
      "metadata": {
        "id": "5d5a333d"
      },
      "outputs": [],
      "source": [
        "## TODO: Fill this in!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "088bdb2b",
      "metadata": {
        "id": "088bdb2b"
      },
      "source": [
        "# Part 5: Train Loop\n",
        "\n",
        "Put it all together!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c82ef51",
      "metadata": {
        "scrolled": true,
        "id": "6c82ef51"
      },
      "outputs": [],
      "source": [
        "## TODO: Fill this in!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26e68267",
      "metadata": {
        "id": "26e68267"
      },
      "source": [
        "# Musical Statement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "107571ef",
      "metadata": {
        "id": "107571ef"
      },
      "outputs": [],
      "source": [
        "## TODO: Fill this in!  Have fun!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}